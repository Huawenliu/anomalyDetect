# Combine scores using Affinity Propagation

import numpy as np
from sklearn.cluster import AffinityPropagation
from scipy.spatial.distance import correlation
try:
    from cy_kendalltau import cy_kendalltau
    CY_KENDALLTAU = True
except:
    from scipy.stats import kendalltau
    CY_KENDALLTAU = False
from sklearn.metrics.pairwise import pairwise_distances

from ..utils import normalize_scores, scores_to_ranks, rank_distances

def combine_scores_ap(scores, method="rank_distances", verbose=False, **kwargs):
    '''
    Input:
        `matrix_of_scores` - a matrix where each column is a vector of scores generated by one method for all the dataset's samples
    '''
    scores = np.asarray(scores)
    n = scores.shape[0]
    
    #matrix_of_rankings = np.argsort(np.argsort(matrix_of_scores, axis=0), axis=0) # larger is more anomalous, so 0 is least anomalous and n is most
    
    rankings = scores_to_ranks(scores)
    if method == "rank_distances":
        S = rank_distances(rankings, k=0.10) 
        S /= S.max()
        S = (1.0 - S)*100
        if verbose: print S
        labels = AffinityPropagation(preference=0.01, affinity="precomputed").fit_predict(S)
    elif method == "correlation":
        S = -pairwise_distances(rankings.T, metric="correlation") + 1  # convert correlation DISTANCE back to normal correlation
        labels = AffinityPropagation(affinity="precomputed").fit_predict(S)
    elif method == "kendalltau":
        # actually pairwise_similarities because kendalltau, 1.0 is perfect match
        if CY_KENDALLTAU:
            S = pairwise_distances(rankings.T,
                                   metric=cy_kendalltau)
        else:
            S = pairwise_distances(rankings.T,
                                   metric=lambda a1, a2: kendalltau(a1, a2)[0])
        labels = AffinityPropagation(affinity="precomputed").fit_predict(S)
    elif method == "precomputed":
        S = kwargs.get('S')
        labels = AffinityPropagation(affinity="precomputed").fit_predict(S)
    else:
        raise Exception("aoeu")
    if labels is None or len(labels) == 0:
        weights = np.ones(scores.shape[1])
    else:
        if verbose: print max(labels)+1, "clusters"
        (_, counts) = np.unique(labels, return_counts=True)
        weights = 1. / counts[labels]
    
    scores = normalize_scores(scores)
    
    new_scores = (scores * weights).sum(axis=1) / np.sum(weights)
    return new_scores